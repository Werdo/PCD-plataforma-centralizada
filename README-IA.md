# ğŸ¤– README - IntegraciÃ³n de Inteligencia Artificial (IA)

Este documento describe cÃ³mo estÃ¡ estructurada y desplegada la capa de servicios de IA en la Plataforma Centralizada.

---

## ğŸ§  Arquitectura actual de IA

La integraciÃ³n IA estÃ¡ pensada en dos fases:

### Fase 1 - IA externa vÃ­a OpenAI
- Un microservicio FastAPI (`openai-proxy`) actÃºa como intermediario seguro entre FYR y ChatGPT (vÃ­a API OpenAI)
- Permite definir la clave OpenAI desde `.env` o variables de entorno
- El frontend FYR solo interactÃºa con el endpoint `/ask` local

### Fase 2 - IA local (on-premise)
- Estructura preparada para desplegar `deepseek-onprem`, un servidor de modelo en contenedor
- Puede integrar modelos como Mistral, DeepSeek, LLaMA o similares vÃ­a Ollama, LM Studio, Text Generation WebUI
- Interfaz expuesta por API HTTP local

---

## ğŸ“‚ Estructura en carpetas

```
scripts/backend-files/ia/
â”œâ”€â”€ openai-proxy/
â”‚   â”œâ”€â”€ main.py
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â”œâ”€â”€ .env.example
â”‚   â””â”€â”€ gateway-openai.yaml
â””â”€â”€ deepseek-onprem/   â† (pendiente de desarrollo)
```

---

## ğŸš€ Despliegue del proxy OpenAI

1. Configura tu `.env`:
```dotenv
OPENAI_API_KEY=sk-xxxxx
OPENAI_MODEL=gpt-3.5-turbo
```

2. Construye e importa:
```bash
./scripts/15-deploy-openai-proxy.sh
```

3. Endpoint disponible:
```
POST http://openai-proxy-service:8010/ask
{ "prompt": "Â¿CuÃ¡l es el vino ideal para maridar con queso azul?" }
```

---

## ğŸŒ ConexiÃ³n desde FYR

FYR debe apuntar a:
```env
VITE_AI_ENDPOINT=http://openai-proxy-service:8010/ask
```

---

## ğŸ› ï¸ Pendiente en IA local

- ConstrucciÃ³n del contenedor `deepseek-onprem`
- IntegraciÃ³n de modelo HuggingFace/Transformers
- Despliegue vÃ­a GPU si estÃ¡ disponible
- ConexiÃ³n con FYR ajustando VITE_AI_ENDPOINT a `http://deepseek-onprem:port/ask`

---

Este sistema garantiza flexibilidad: primero con OpenAI, luego con tu propia IA privada.

